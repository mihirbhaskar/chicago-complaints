{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4818a5a6",
   "metadata": {},
   "source": [
    "# Cleaning the Chicago Police Investigatory Stops Data\n",
    "#### Mihir Bhaskar\n",
    "#### 11/28/2021\n",
    "\n",
    "The following file reads in raw .csv data files on Investigatory Stops by the Chicago Police Department sourced from: https://home.chicagopolice.org/statistics-data/isr-data/ (accessed on 28th November, 2021). The files for 2016, 2017 and 2018-19 were all individually downloaded as .csv files.\n",
    "\n",
    "This file then does the following:\n",
    "1. Imports and appends the year-wise data\n",
    "2. Cleans stop reports by removing duplicates\n",
    "3. Aggregates stops to the police beat level, and then merges these with police beat boundaries sourced from Chicago's Open Data Portal at this link: https://data.cityofchicago.org/Public-Safety/Boundaries-Police-Beats-current-/aerh-rz74. Note that the data is accessed using the Socrata API directly\n",
    "4. Imports the CleanACSFile data outputted from 2_CleanACS to get the tract ID and polygons for each census tract\n",
    "5. Does a spatial join to map each beat boundary to the tracts that overlap with it spatially\n",
    "6. Distribute and aggregate the stop counts so that we end up with a dataset of each tract, and the number of stops associated with it.\n",
    "\n",
    "It then exports a .csv file called 'CleanStopReports', which has all the tract IDs in Chicago, along with columns relating to the number of stops (e.g. total # of stops made in each tract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f455155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from pyprojroot import here\n",
    "from sodapy import Socrata\n",
    "from shapely.geometry import shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e37bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacar\\anaconda3\\lib\\site-packages\\pyprojroot\\pyprojroot.py:51: UserWarning: Path doesn't exist: C:\\Users\\jacar\\OneDrive\\Documents\\chicago-complaints\\data\\raw\\ISR_2016.csv\n",
      "  warnings.warn(\"Path doesn't exist: {}\".format(path))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jacar\\\\OneDrive\\\\Documents\\\\chicago-complaints\\\\data\\\\raw\\\\ISR_2016.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0df884ac4c70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import data (downloaded from website linked above as .csv files)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0misr_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/raw/ISR_2016.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0misr_17\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/raw/ISR_2017.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0misr_1819\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/raw/ISR_2018-2019.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jacar\\\\OneDrive\\\\Documents\\\\chicago-complaints\\\\data\\\\raw\\\\ISR_2016.csv'"
     ]
    }
   ],
   "source": [
    "# Import data (downloaded from website linked above as .csv files)\n",
    "\n",
    "isr_16 = pd.read_csv(here('./data/raw/ISR_2016.csv'))\n",
    "isr_17 = pd.read_csv(here('./data/raw/ISR_2017.csv'))\n",
    "isr_1819 = pd.read_csv(here('./data/raw/ISR_2018-2019.csv'))\n",
    "\n",
    "# Appending data from multiple years into one dataframe\n",
    "isr_df = isr_16.append(isr_17)\n",
    "isr_df = isr_df.append(isr_1819)\n",
    "\n",
    "print(isr_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08131eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isr_df.describe()\n",
    "#isr_df.info()\n",
    "#isr_df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc65fb",
   "metadata": {},
   "source": [
    "### Creating a simple dataset of stop report counts per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only relevant variables\n",
    "basic_df = isr_df[['CARD_NO', 'BEAT']]\n",
    "\n",
    "# Dropping duplicates in CARD_NO\n",
    "# Note: according to the data description (found on the same website from which data was sourced), multiple card numbers \n",
    "# basically mean that the same incident may have had updated information. Since for now we are only interested in total number\n",
    "# of incidents, we can drop the duplicates\n",
    "\n",
    "basic_df.drop_duplicates(subset=['CARD_NO'], inplace=True)\n",
    "\n",
    "# Checking the quality/missingness of beat data\n",
    "print(basic_df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import beat boundaries from the Chicago Open Data Portal - the code below\n",
    "# is sourced from the following API documentations on the Chicago Open Data Portal: https://dev.socrata.com/foundry/data.cityofchicago.org/n9it-hstw\n",
    "\n",
    "client = Socrata(\"data.cityofchicago.org\", None)\n",
    "\n",
    "# Fetch the results\n",
    "beat_bounds = client.get(\"n9it-hstw\", limit=2000)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "beat_bounds = pd.DataFrame.from_records(beat_bounds)\n",
    "\n",
    "# Convert the beat number to numeric for merging \n",
    "beat_bounds['beat_num'] = pd.to_numeric(beat_bounds['beat_num'])\n",
    "\n",
    "beat_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the count of stops to the beat boundaries dataset\n",
    "\n",
    "# Aggregate stop counts to the beat level\n",
    "basic_df = basic_df.groupby('BEAT').count()\n",
    "\n",
    "# Merge these datasets on the beat number\n",
    "basic_df = beat_bounds.merge(basic_df, how='left', left_on=['beat_num'], right_on=['BEAT'])\n",
    "\n",
    "# Convert the dataset to a geodataframe (needed for spatial merge)\n",
    "basic_df['the_geom'] = basic_df['the_geom'].apply(shape)\n",
    "basic_df = gpd.GeoDataFrame(basic_df, geometry='the_geom', crs='epsg:4326')\n",
    "\n",
    "basic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e748a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing beats that didn't merge with any stops to have 0 counts\n",
    "basic_df.isnull().sum()\n",
    "\n",
    "basic_df['CARD_NO'].describe()\n",
    "\n",
    "## We find that actually every beat has a minimum of 144 stops - that is, every beat in the police beat boundaries data matched to \n",
    "## beats from our stop reports data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb19fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the census tract boundaries to do a spatial merge, mapping the police beats to census tracts\n",
    "\n",
    "# Importing the census tracts data and converting it to a GeoDataFrame\n",
    "acs = pd.read_csv(here('./data/CleanACSFile.csv'))\n",
    "\n",
    "# Keeping only relevant info from the ACS file for the spatial merge\n",
    "acs = acs[['geo_id', 'geometry']]\n",
    "\n",
    "# Converting the ACS file to a geodataframe\n",
    "acs['geometry'] = acs['geometry'].apply(wkt.loads)\n",
    "acs = gpd.GeoDataFrame(acs, crs='epsg:4326')\n",
    "\n",
    "# Doing the spatial merge to assign a tract ID to every complaint\n",
    "basic_df = gpd.sjoin(basic_df, acs[['geo_id', 'geometry']], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de528c",
   "metadata": {},
   "source": [
    "**Now, we have a dataframe where every beat has multiple rows, because a specific beat maps to many different census tracts.** The methodology for resolving these and getting down to a unique tract-level database is as follows:\n",
    "1. For every beat, evenly divide up the number of stops for each tract that matches to it\n",
    "2. Aggregate up the stops at a tract-level, so that if a tract exists in multiple beats, the stops associated with it from each beat is added up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing up the stops in every beat to the different tracts that match to it\n",
    "basic_df['matching_tract_count'] = basic_df['geo_id'].groupby(basic_df['beat_num']).transform('count')\n",
    "basic_df['assigned_stops'] = basic_df['CARD_NO'] / basic_df['matching_tract_count']\n",
    "\n",
    "# Aggregating up the assigned stops for each geo_id (i.e. each census tract)\n",
    "basic_df['inv_stops_pertract'] = basic_df['matching_tract_count'].groupby(basic_df['geo_id']).transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates to now get the dataset down to the tract level\n",
    "basic_df.drop_duplicates(subset=['geo_id'], inplace=True)\n",
    "\n",
    "# Keeping relevant variables\n",
    "basic_df = basic_df[['geo_id', 'inv_stops_pertract']]\n",
    "basic_df.rename(columns={'inv_stops_pertract':'inv_stop_count'}, inplace=True)\n",
    "\n",
    "# Merging these counts back with the full dataset of census tract IDs\n",
    "merged = acs.merge(basic_df, how='left', on=['geo_id'])\n",
    "\n",
    "# Replacing missing crime ID count values with 0 (i.e. missing means there were 0 complaints found in that tract)\n",
    "merged['inv_stop_count'] = merged['inv_stop_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76693790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export .CSV file to be used in other scripts\n",
    "merged[['geo_id', 'inv_stop_count']].to_csv(here('./data/CleanStopReports.csv'),\n",
    "                                            encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
